
"""Qwen_VL (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sOxLW4PY8TwvfLBRnEpypAhHaliPzrUF
"""

!pip install -U bitsandbytes
!pip install pycocoevalcap bert-score spacy
!python -m spacy download en_core_web_sm
!pip install transformers torch torchvision datasets pillow accelerate bitsandbytes qwen-vl-utils
!pip install rouge-score nltk sacrebleu sentence-transformers
!pip install matplotlib seaborn pandas numpy

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from datasets import load_dataset
import pandas as pd
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
from rouge_score import rouge_scorer
from sacrebleu.metrics import BLEU
import nltk
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import io
import gc

warnings.filterwarnings('ignore')

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name()}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

def clear_memory():
    """Clear GPU memory to prevent OOM errors"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# Loading Image Caption Dataset
dataset = load_dataset("jaimin/Image_Caption")
df = dataset['train'].to_pandas()

print(f"Dataset Shape: {df.shape}")
print("\nDataset Columns:")
for col in df.columns:
    print(f"  â€¢ {col}: {df[col].dtype}")

print("\nFirst 5 rows:")
print(df.head())

model_name = "Qwen/Qwen2-VL-7B-Instruct"

processor = AutoProcessor.from_pretrained(model_name)
model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

clear_memory()
print("Model loaded successfully!")

def generate_caption(image, prompt="Describe this image in detail."):
    """
    Generate caption using Qwen2-VL model

    Args:
        image (PIL.Image): Input image
        prompt (str): Text prompt for the model

    Returns:
        str: Generated caption
    """
    try:
        # Resize image if too large to save memory
        max_size = 1024
        if max(image.size) > max_size:
            ratio = max_size / max(image.size)
            new_size = tuple(int(dim * ratio) for dim in image.size)
            image = image.resize(new_size, Image.Resampling.LANCZOS)

        # Prepare the conversation format expected by Qwen2-VL
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image,
                    },
                    {"type": "text", "text": prompt},
                ],
            }
        ]

        # Process the input
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(device)

        # Generate response
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=100,  # Limit tokens to save memory
                do_sample=False,
                pad_token_id=processor.tokenizer.pad_token_id
            )

        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]

        caption = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        clear_memory()  # Clear memory after generation
        return caption.strip()

    except Exception as e:
        clear_memory()
        return f"Error: {str(e)}"

test_size = 20
test_df = df.iloc[:test_size].copy()

predictions = []
ground_truths = []

for idx, row in test_df.iterrows():
    print(f"Processing image {idx+1}/{test_size}...", end='\r')

    # Load image from bytes (format: {'bytes': b'\xff\xd8\xff\xe0...})
    image_bytes = row['image']['bytes']
    image = Image.open(io.BytesIO(image_bytes)).convert('RGB')

    # Generate caption using Qwen2-VL
    pred_caption = generate_caption(image)

    predictions.append(pred_caption)
    ground_truths.append(row['text'])

test_df['qwen_prediction'] = predictions
print(f"\nCaption generation completed!")

for i in range(min(5, len(test_df))):
    print(f"\nImage {i+1}:")
    print(f"Ground Truth: {test_df.iloc[i]['text']}")
    print(f"Qwen2-VL Prediction: {test_df.iloc[i]['qwen_prediction']}")
    print("-" * 60)

test_df.head(150)

# Save results
test_df.to_csv("qwen_test_df_output.csv", index=False)
print("Saved test_df to qwen_test_df_output.csv")

