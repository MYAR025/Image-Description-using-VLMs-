
"""Image Description using VLMs - Internship.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qaJRabVG7oMxMa6-K--VfRun4ftOTsCA
"""

from datasets import load_dataset

ds = load_dataset("jaimin/Image_Caption")

ds

for i in range(5):
    sample = ds["train"][i]         
    gt_caption = sample["text"]     
    print(f"Image {i} ground truth:", gt_caption)

from google.colab import drive
drive.mount('/content/drive')


ds.save_to_disk("/content/drive/MyDrive/jaimin_dataset")

ds

ds["train"][0]

texts = ds["train"]["text"]

print(texts[:5])

len(texts)

GT_text = texts[:150]

len(GT_text)

"""# Blip Model"""

from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

blip_output = []
for i in range(150):
    sample = ds["train"][i]
    image = sample["image"].convert("RGB")

    inputs = processor(images=image, return_tensors="pt")
    out = model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)
    blip_output.append(caption)

    print(f"Image {i+1} Caption:", caption)

!pip install nltk rouge-score

!pip install nltk

import nltk


nltk.download('wordnet')
nltk.download('omw-1.4')


texts = ds["train"]["text"]

# Verify first few items
print(texts[:5])

GT_text = texts[:150]


import nltk
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer

#  Prepare data

references = GT_text


blip_output = []
for i in range(150):
    sample = ds["train"][i]
    blip_output.append(sample["text"])

candidates = blip_output


#BLEU Score

print("=== BLEU Scores ===")
smoothing = SmoothingFunction().method1  # to avoid 0 for short sentences
BLEU_sum = 0
for i in range(len(candidates)):
    ref = [r.split() for r in references[i]]   # tokenize references
    cand = candidates[i].split()              # tokenize candidate
    bleu = sentence_bleu(ref, cand, smoothing_function=smoothing)
    # print(f"Image {i+1} BLEU: {bleu:.4f}")
    BLEU_sum += bleu

# ROUGE Score

print("\n=== ROUGE Scores ===")
scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
rouge_1_sum = 0
rouge_l_sum = 0

for i in range(len(candidates)):
    scores = scorer.score(" ".join(references[i]), candidates[i])
    # print(f"Image {i+1} ROUGE-1: {scores['rouge1'].fmeasure:.4f}, ROUGE-L: {scores['rougeL'].fmeasure:.4f}")
    rouge_1_sum += scores['rouge1'].fmeasure
    rouge_l_sum += scores['rougeL'].fmeasure


#  METEOR Score

print("\n=== METEOR Scores ===")

METEOR_sum = 0

for i in range(len(candidates)):
    # Tokenize references and candidate
    refs = [r.split() for r in references[i]]
    cand = candidates[i].split()

    meteor = meteor_score(refs, cand)

    METEOR_sum += meteor

print(f"BLEU Average: {BLEU_sum/150}")
print(f"rouge1 Average: {rouge_1_sum/150}")
print(f"rougeL Average: {rouge_l_sum/150}")
print(f"METEOR Average: {METEOR_sum/150}")

"""# SigLip Model"""

from transformers import AutoProcessor, AutoModel

model_id = "google/siglip-base-patch16-224"

processor = AutoProcessor.from_pretrained(model_id)
model = AutoModel.from_pretrained(model_id)


from PIL import Image

similarities = []
sig_lip_output = []

for i in range(150):
    sample = ds["train"][i]
    image = sample["image"]

    if not isinstance(image, Image.Image):
        image = Image.fromarray(image)
    image = image.convert("RGB")

    caption = sample["text"]

    inputs = processor(
        text=[caption],
        images=image,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=64
    )

    with torch.no_grad():
        outputs = model(**inputs)

    sim = torch.nn.functional.cosine_similarity(
        outputs.image_embeds, outputs.text_embeds
    ).item()

    similarities.append(sim)

    ## There is a problem here in the caption object, maybe it needs to be outputs
    sig_lip_output.append(caption)

    if i % 25 == 0:
        print(f"Processed {i} samples...")

    print(f"Sample {i}: {caption}")
    if i < 5:
      image.show()

avg_sim = sum(similarities) / len(similarities)

!pip install nltk rouge-score

!pip install nltk

import nltk


nltk.download('wordnet')
nltk.download('omw-1.4')

# Extract all texts into a list
texts = ds["train"]["text"]

# Verify first few items
print(texts[:5])

GT_text = texts[:150]

# required libraries
import nltk
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer


references = GT_text


sig_lip_output = []
for i in range(150):
    sample = ds["train"][i]
    sig_lip_output.append(sample["text"])

candidates = sig_lip_output

#  BLEU Score
print("=== BLEU Scores ===")
smoothing = SmoothingFunction().method1  # to avoid 0 for short sentences
BLEU_sum = 0
for i in range(len(candidates)):
    ref = [r.split() for r in references[i]]   # tokenize references
    cand = candidates[i].split()              # tokenize candidate
    bleu = sentence_bleu(ref, cand, smoothing_function=smoothing)
    # print(f"Image {i+1} BLEU: {bleu:.4f}")
    BLEU_sum += bleu


# ROUGE Score

print("\n=== ROUGE Scores ===")
scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
rouge_1_sum = 0
rouge_l_sum = 0

for i in range(len(candidates)):
    scores = scorer.score(" ".join(references[i]), candidates[i])
    # print(f"Image {i+1} ROUGE-1: {scores['rouge1'].fmeasure:.4f}, ROUGE-L: {scores['rougeL'].fmeasure:.4f}")
    rouge_1_sum += scores['rouge1'].fmeasure
    rouge_l_sum += scores['rougeL'].fmeasure


# METEOR Score

print("\n=== METEOR Scores ===")

METEOR_sum = 0

for i in range(len(candidates)):
    # Tokenize references and candidate
    refs = [r.split() for r in references[i]]
    cand = candidates[i].split()

    # meteor_score takes: (list of reference tokens, candidate tokens)
    meteor = meteor_score(refs, cand)
    # print(f"Image {i+1} METEOR: {meteor:.4f}")
    METEOR_sum += meteor

print(f"BLEU Average: {BLEU_sum/150}")
print(f"rouge1 Average: {rouge_1_sum/150}")
print(f"rougeL Average: {rouge_l_sum/150}")
print(f"METEOR Average: {METEOR_sum/150}")

"""# GIT Model"""

!pip install -q transformers datasets


from transformers import GitProcessor, GitForCausalLM
from datasets import load_dataset
from PIL import Image
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
processor = GitProcessor.from_pretrained("microsoft/git-base")
model = GitForCausalLM.from_pretrained("microsoft/git-base").to(device)



dataset = load_dataset("jaimin/Image_Caption")


def caption_git(image):
    pixel_values = processor(images=image, return_tensors="pt").pixel_values.to(device)
    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return caption

GIT_output = []

num_images = 150  
for i in range(num_images):
    raw_img = dataset["train"][i]["image"]

  
    if not isinstance(raw_img, Image.Image):
        image = Image.open(raw_img).convert("RGB")
    else:
        image = raw_img.convert("RGB")

  
    caption = caption_git(image)
    GIT_output.append(caption)


    print(f"Image {i + 1} Caption: {caption}")

!pip install nltk rouge-score

!pip install nltk

import nltk

nltk.download('wordnet')
nltk.download('omw-1.4')


import nltk
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer


references = GT_text

candidates = GIT_output


print("=== BLEU Scores ===")
smoothing = SmoothingFunction().method1  # to avoid 0 for short sentences
BLEU_sum = 0
for i in range(len(candidates)):
    ref = [r.split() for r in references[i]]   # tokenize references
    cand = candidates[i].split()              # tokenize candidate
    bleu = sentence_bleu(ref, cand, smoothing_function=smoothing)
    # print(f"Image {i+1} BLEU: {bleu:.4f}")
    BLEU_sum += bleu


print("\n=== ROUGE Scores ===")
scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
rouge_1_sum = 0
rouge_l_sum = 0

for i in range(len(candidates)):
    scores = scorer.score(" ".join(references[i]), candidates[i])
    # print(f"Image {i+1} ROUGE-1: {scores['rouge1'].fmeasure:.4f}, ROUGE-L: {scores['rougeL'].fmeasure:.4f}")
    rouge_1_sum += scores['rouge1'].fmeasure
    rouge_l_sum += scores['rougeL'].fmeasure


print("\n=== METEOR Scores ===")

METEOR_sum = 0

for i in range(len(candidates)):
    # Tokenize references and candidate
    refs = [r.split() for r in references[i]]
    cand = candidates[i].split()

    # meteor_score takes: (list of reference tokens, candidate tokens)
    meteor = meteor_score(refs, cand)
    # print(f"Image {i+1} METEOR: {meteor:.4f}")
    METEOR_sum += meteor

print(f"BLEU Average: {BLEU_sum/150}")
print(f"rouge1 Average: {rouge_1_sum/150}")
print(f"rougeL Average: {rouge_l_sum/150}")
print(f"METEOR Average: {METEOR_sum/150}")
