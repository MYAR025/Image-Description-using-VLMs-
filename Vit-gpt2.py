# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zfqn6Fix_bY4plGaWadYCtHMNE48Cuo1
"""

from datasets import load_dataset

ds = load_dataset("jaimin/Image_Caption")

ds

for i in range(5):
    sample = ds["train"][i]         # take the i-th sample
    gt_caption = sample["text"]     # ground truth text
    print(f"Image {i} ground truth:", gt_caption)

from google.colab import drive
drive.mount('/content/drive')

# Save the datasetdict to Google Drive
ds.save_to_disk("/content/drive/MyDrive/jaimin_dataset")

ds

ds["train"][0]

# Extract all texts into a list
texts = ds["train"]["text"]

# Verify first few items
print(texts[:5])

len(texts)

GT_text = texts[:150]

len(GT_text)

from datasets import load_dataset
from PIL import Image
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import torch

# Load your dataset
ds = load_dataset("jaimin/Image_Caption", split="train")

# Load model and processor
model_id = "ydshieh/vit-gpt2-coco-en"
model = VisionEncoderDecoderModel.from_pretrained(model_id)
processor = ViTImageProcessor.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Caption generation function
def generate_caption(image: Image.Image, max_length=16):
    if not isinstance(image, Image.Image):
        image = Image.fromarray(image)
    image = image.convert("RGB")

    pixel_values = processor(images=image, return_tensors="pt").pixel_values.to(device)

    output_ids = model.generate(pixel_values, max_length=max_length, num_beams=4)
    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return caption.strip()

# Run for first 10 samples
for i in range(150):
    image = ds[i]["image"]
    caption = generate_caption(image)
    print(f"[{i}] Caption: {caption}")